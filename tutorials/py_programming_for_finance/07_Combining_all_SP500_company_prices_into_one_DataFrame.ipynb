{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d7dabbf",
   "metadata": {},
   "source": [
    "## Combining all S&P 500 company prices into one DataFrame\n",
    "\n",
    "https://pythonprogramming.net/combining-stock-prices-into-one-dataframe-python-programming-for-finance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65a502bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker.strip())\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "# save_sp500_tickers()\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        # e.g. BRK.B => BRK-B, BF.B => BF-B\n",
    "        ticker = ticker.replace('.', '-')\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = yf.download(ticker, start=start, end=end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "# get_data_from_yahoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebd5043",
   "metadata": {},
   "source": [
    "While we do have all of the data at our disposal, we may want to actually assess the data together. To do this, we're going to join all of the stock datasets together. Each of the stock files at the moment come with: `Open`, `High`, `Low`, `Close`, `Volume`, and `Adj Close`. At least to start, we're mostly just interested in the adjusted close for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b1a0892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compile_data():\n",
    "#     with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "#         tickers = pickle.load(f)\n",
    "\n",
    "#     main_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f730a",
   "metadata": {},
   "source": [
    "To begin, we pull our previously-made list of tickers, and begin with an empty DataFrame, called main_df. Now, we're ready to read in each stock's dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "533bd2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     for count,ticker in enumerate(tickers):\n",
    "#         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "#         df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e11d",
   "metadata": {},
   "source": [
    "You do not need to use [Python's enumerate](https://pythonprogramming.net/enumerate-intermediate-python-tutorial/) here, I am just using it so we know where we are in the process of reading in all of the data. You could just iterate over the tickers. From this point, we **could** generate extra columns with interesting data, like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5fd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         df['{}_HL_pct_diff'.format(ticker)] = (df['High'] - df['Low']) / df['Low']\n",
    "#         df['{}_daily_pct_chng'.format(ticker)] = (df['Close'] - df['Open']) / df['Open']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa12aca",
   "metadata": {},
   "source": [
    "For now, however, we're not going to be bothered with this. Just know this could be a path to pursue down the road. Instead, we're really just interested in that `Adj Close` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e424a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         df.rename(columns={'Adj Close':ticker}, inplace=True)\n",
    "#         df.drop(['Open','High','Low','Close','Volume'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada3f938",
   "metadata": {},
   "source": [
    "Now we've got just that column (or maybe extras, like above...but remember, in this example, we're not doing the `HL_pct_diff` or `daily_pct_chng`). Notice that we have renamed the `Adj Close` column to whatever the ticker name is. Let's begin building the shared dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "355369c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if main_df.empty:\n",
    "#             main_df = df\n",
    "#         else:\n",
    "#             main_df = main_df.join(df, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2552bae3",
   "metadata": {},
   "source": [
    "If there's nothing in the main_df, then we'll start with the current df, otherwise we're going to use [Pandas' join](https://pythonprogramming.net/join-merge-data-analysis-python-pandas-tutorial/).\n",
    "\n",
    "Still within this for loop, we'll add two more lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68afdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         if count % 10 == 0:\n",
    "#             print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c860912",
   "metadata": {},
   "source": [
    "This will just output the count of the current ticker if it's evenly divisible by 10. What `count % 10` gives us is the remainder if count was to be divided by 10. So if we ask `if count % 10 == 0`, we're only going to see the if statement True if the current count, divided by 10, has a remainder of 0, or if it is perfectly divisible by 10.\n",
    "\n",
    "When we're all done with the for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c57d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     print(main_df.head())\n",
    "#     main_df.to_csv('sp500_joined_closes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7b8b9f",
   "metadata": {},
   "source": [
    "This function and calling it up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af5fda2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "#         tickers = pickle.load(f)\n",
    "\n",
    "#     main_df = pd.DataFrame()\n",
    "\n",
    "#     for count,ticker in enumerate(tickers):\n",
    "#         df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "#         df.set_index('Date', inplace=True)\n",
    "\n",
    "#         df.rename(columns={'Adj Close':ticker}, inplace=True)\n",
    "#         df.drop(['Open','High','Low','Close','Volume'],1,inplace=True)\n",
    "\n",
    "#         if main_df.empty:\n",
    "#             main_df = df\n",
    "#         else:\n",
    "#             main_df = main_df.join(df, how='outer')\n",
    "\n",
    "#         if count % 10 == 0:\n",
    "#             print(count)\n",
    "#     print(main_df.head())\n",
    "#     main_df.to_csv('sp500_joined_closes.csv')\n",
    "\n",
    "\n",
    "# compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3803f40",
   "metadata": {},
   "source": [
    "Full code up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b89658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 503/503 [00:02<00:00, 193.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MMM</th>\n",
       "      <th>AOS</th>\n",
       "      <th>ABT</th>\n",
       "      <th>ABBV</th>\n",
       "      <th>ACN</th>\n",
       "      <th>ADM</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>ADP</th>\n",
       "      <th>AES</th>\n",
       "      <th>AFL</th>\n",
       "      <th>...</th>\n",
       "      <th>WTW</th>\n",
       "      <th>GWW</th>\n",
       "      <th>WYNN</th>\n",
       "      <th>XEL</th>\n",
       "      <th>XYL</th>\n",
       "      <th>YUM</th>\n",
       "      <th>ZBRA</th>\n",
       "      <th>ZBH</th>\n",
       "      <th>ZION</th>\n",
       "      <th>ZTS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>55.033230</td>\n",
       "      <td>6.061760</td>\n",
       "      <td>19.138166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.455265</td>\n",
       "      <td>22.142736</td>\n",
       "      <td>37.090000</td>\n",
       "      <td>27.043221</td>\n",
       "      <td>9.865004</td>\n",
       "      <td>16.935186</td>\n",
       "      <td>...</td>\n",
       "      <td>53.240776</td>\n",
       "      <td>76.363777</td>\n",
       "      <td>41.758171</td>\n",
       "      <td>13.167749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.345045</td>\n",
       "      <td>28.670000</td>\n",
       "      <td>52.791035</td>\n",
       "      <td>10.726543</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>54.688534</td>\n",
       "      <td>5.984308</td>\n",
       "      <td>18.983543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.655857</td>\n",
       "      <td>22.262346</td>\n",
       "      <td>37.700001</td>\n",
       "      <td>26.897989</td>\n",
       "      <td>9.763973</td>\n",
       "      <td>17.426476</td>\n",
       "      <td>...</td>\n",
       "      <td>53.121429</td>\n",
       "      <td>76.395195</td>\n",
       "      <td>44.297863</td>\n",
       "      <td>13.011587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.278898</td>\n",
       "      <td>28.620001</td>\n",
       "      <td>54.462196</td>\n",
       "      <td>11.104751</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>55.464123</td>\n",
       "      <td>5.987023</td>\n",
       "      <td>19.088966</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.002991</td>\n",
       "      <td>22.206062</td>\n",
       "      <td>37.619999</td>\n",
       "      <td>26.834860</td>\n",
       "      <td>9.662939</td>\n",
       "      <td>17.579561</td>\n",
       "      <td>...</td>\n",
       "      <td>53.976639</td>\n",
       "      <td>76.685776</td>\n",
       "      <td>43.716812</td>\n",
       "      <td>13.036573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.141064</td>\n",
       "      <td>28.400000</td>\n",
       "      <td>54.444588</td>\n",
       "      <td>12.070378</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>55.503906</td>\n",
       "      <td>6.004689</td>\n",
       "      <td>19.247101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.972153</td>\n",
       "      <td>21.973869</td>\n",
       "      <td>36.889999</td>\n",
       "      <td>26.822224</td>\n",
       "      <td>9.670161</td>\n",
       "      <td>17.768244</td>\n",
       "      <td>...</td>\n",
       "      <td>53.817532</td>\n",
       "      <td>77.408264</td>\n",
       "      <td>44.650425</td>\n",
       "      <td>12.980354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.135551</td>\n",
       "      <td>27.690001</td>\n",
       "      <td>55.693562</td>\n",
       "      <td>13.422259</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>55.894970</td>\n",
       "      <td>6.093013</td>\n",
       "      <td>19.345501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.841000</td>\n",
       "      <td>21.699457</td>\n",
       "      <td>36.689999</td>\n",
       "      <td>26.784344</td>\n",
       "      <td>9.944386</td>\n",
       "      <td>17.590239</td>\n",
       "      <td>...</td>\n",
       "      <td>53.757862</td>\n",
       "      <td>78.319237</td>\n",
       "      <td>44.330513</td>\n",
       "      <td>12.986597</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.141064</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>54.523750</td>\n",
       "      <td>13.204995</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-24</th>\n",
       "      <td>90.120003</td>\n",
       "      <td>66.639999</td>\n",
       "      <td>94.809998</td>\n",
       "      <td>146.309998</td>\n",
       "      <td>296.089996</td>\n",
       "      <td>69.470001</td>\n",
       "      <td>539.559998</td>\n",
       "      <td>240.449997</td>\n",
       "      <td>14.530000</td>\n",
       "      <td>77.760002</td>\n",
       "      <td>...</td>\n",
       "      <td>207.889999</td>\n",
       "      <td>686.710022</td>\n",
       "      <td>91.419998</td>\n",
       "      <td>59.080002</td>\n",
       "      <td>89.440002</td>\n",
       "      <td>119.910004</td>\n",
       "      <td>205.910004</td>\n",
       "      <td>104.839996</td>\n",
       "      <td>29.860001</td>\n",
       "      <td>167.119995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-25</th>\n",
       "      <td>89.389999</td>\n",
       "      <td>65.190002</td>\n",
       "      <td>93.570000</td>\n",
       "      <td>145.259995</td>\n",
       "      <td>292.679993</td>\n",
       "      <td>71.050003</td>\n",
       "      <td>521.140015</td>\n",
       "      <td>218.330002</td>\n",
       "      <td>14.680000</td>\n",
       "      <td>78.260002</td>\n",
       "      <td>...</td>\n",
       "      <td>207.740005</td>\n",
       "      <td>677.409973</td>\n",
       "      <td>88.940002</td>\n",
       "      <td>59.470001</td>\n",
       "      <td>87.760002</td>\n",
       "      <td>120.309998</td>\n",
       "      <td>198.910004</td>\n",
       "      <td>103.639999</td>\n",
       "      <td>29.629999</td>\n",
       "      <td>163.669998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-26</th>\n",
       "      <td>88.650002</td>\n",
       "      <td>69.120003</td>\n",
       "      <td>93.980003</td>\n",
       "      <td>145.199997</td>\n",
       "      <td>292.040009</td>\n",
       "      <td>71.849998</td>\n",
       "      <td>514.280029</td>\n",
       "      <td>218.839996</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>77.519997</td>\n",
       "      <td>...</td>\n",
       "      <td>229.080002</td>\n",
       "      <td>699.669983</td>\n",
       "      <td>87.540001</td>\n",
       "      <td>59.770000</td>\n",
       "      <td>88.690002</td>\n",
       "      <td>118.750000</td>\n",
       "      <td>204.830002</td>\n",
       "      <td>103.120003</td>\n",
       "      <td>30.450001</td>\n",
       "      <td>158.070007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-27</th>\n",
       "      <td>87.519997</td>\n",
       "      <td>67.809998</td>\n",
       "      <td>92.849998</td>\n",
       "      <td>138.929993</td>\n",
       "      <td>290.040009</td>\n",
       "      <td>70.040001</td>\n",
       "      <td>508.119995</td>\n",
       "      <td>214.839996</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>76.489998</td>\n",
       "      <td>...</td>\n",
       "      <td>228.919998</td>\n",
       "      <td>706.760010</td>\n",
       "      <td>87.279999</td>\n",
       "      <td>58.310001</td>\n",
       "      <td>88.169998</td>\n",
       "      <td>119.440002</td>\n",
       "      <td>207.179993</td>\n",
       "      <td>103.190002</td>\n",
       "      <td>29.450001</td>\n",
       "      <td>156.029999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-30</th>\n",
       "      <td>89.519997</td>\n",
       "      <td>69.489998</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>141.889999</td>\n",
       "      <td>292.700012</td>\n",
       "      <td>71.419998</td>\n",
       "      <td>526.940002</td>\n",
       "      <td>216.080002</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>77.400002</td>\n",
       "      <td>...</td>\n",
       "      <td>232.789993</td>\n",
       "      <td>726.059998</td>\n",
       "      <td>88.080002</td>\n",
       "      <td>58.740002</td>\n",
       "      <td>89.269997</td>\n",
       "      <td>119.870003</td>\n",
       "      <td>209.770004</td>\n",
       "      <td>103.410004</td>\n",
       "      <td>29.980000</td>\n",
       "      <td>157.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3480 rows × 503 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MMM        AOS        ABT        ABBV         ACN  \\\n",
       "Date                                                                  \n",
       "2010-01-04  55.033230   6.061760  19.138166         NaN   32.455265   \n",
       "2010-01-05  54.688534   5.984308  18.983543         NaN   32.655857   \n",
       "2010-01-06  55.464123   5.987023  19.088966         NaN   33.002991   \n",
       "2010-01-07  55.503906   6.004689  19.247101         NaN   32.972153   \n",
       "2010-01-08  55.894970   6.093013  19.345501         NaN   32.841000   \n",
       "...               ...        ...        ...         ...         ...   \n",
       "2023-10-24  90.120003  66.639999  94.809998  146.309998  296.089996   \n",
       "2023-10-25  89.389999  65.190002  93.570000  145.259995  292.679993   \n",
       "2023-10-26  88.650002  69.120003  93.980003  145.199997  292.040009   \n",
       "2023-10-27  87.519997  67.809998  92.849998  138.929993  290.040009   \n",
       "2023-10-30  89.519997  69.489998  93.000000  141.889999  292.700012   \n",
       "\n",
       "                  ADM        ADBE         ADP        AES        AFL  ...  \\\n",
       "Date                                                                 ...   \n",
       "2010-01-04  22.142736   37.090000   27.043221   9.865004  16.935186  ...   \n",
       "2010-01-05  22.262346   37.700001   26.897989   9.763973  17.426476  ...   \n",
       "2010-01-06  22.206062   37.619999   26.834860   9.662939  17.579561  ...   \n",
       "2010-01-07  21.973869   36.889999   26.822224   9.670161  17.768244  ...   \n",
       "2010-01-08  21.699457   36.689999   26.784344   9.944386  17.590239  ...   \n",
       "...               ...         ...         ...        ...        ...  ...   \n",
       "2023-10-24  69.470001  539.559998  240.449997  14.530000  77.760002  ...   \n",
       "2023-10-25  71.050003  521.140015  218.330002  14.680000  78.260002  ...   \n",
       "2023-10-26  71.849998  514.280029  218.839996  14.900000  77.519997  ...   \n",
       "2023-10-27  70.040001  508.119995  214.839996  14.750000  76.489998  ...   \n",
       "2023-10-30  71.419998  526.940002  216.080002  14.950000  77.400002  ...   \n",
       "\n",
       "                   WTW         GWW       WYNN        XEL        XYL  \\\n",
       "Date                                                                  \n",
       "2010-01-04   53.240776   76.363777  41.758171  13.167749        NaN   \n",
       "2010-01-05   53.121429   76.395195  44.297863  13.011587        NaN   \n",
       "2010-01-06   53.976639   76.685776  43.716812  13.036573        NaN   \n",
       "2010-01-07   53.817532   77.408264  44.650425  12.980354        NaN   \n",
       "2010-01-08   53.757862   78.319237  44.330513  12.986597        NaN   \n",
       "...                ...         ...        ...        ...        ...   \n",
       "2023-10-24  207.889999  686.710022  91.419998  59.080002  89.440002   \n",
       "2023-10-25  207.740005  677.409973  88.940002  59.470001  87.760002   \n",
       "2023-10-26  229.080002  699.669983  87.540001  59.770000  88.690002   \n",
       "2023-10-27  228.919998  706.760010  87.279999  58.310001  88.169998   \n",
       "2023-10-30  232.789993  726.059998  88.080002  58.740002  89.269997   \n",
       "\n",
       "                   YUM        ZBRA         ZBH       ZION         ZTS  \n",
       "Date                                                                   \n",
       "2010-01-04   19.345045   28.670000   52.791035  10.726543         NaN  \n",
       "2010-01-05   19.278898   28.620001   54.462196  11.104751         NaN  \n",
       "2010-01-06   19.141064   28.400000   54.444588  12.070378         NaN  \n",
       "2010-01-07   19.135551   27.690001   55.693562  13.422259         NaN  \n",
       "2010-01-08   19.141064   27.600000   54.523750  13.204995         NaN  \n",
       "...                ...         ...         ...        ...         ...  \n",
       "2023-10-24  119.910004  205.910004  104.839996  29.860001  167.119995  \n",
       "2023-10-25  120.309998  198.910004  103.639999  29.629999  163.669998  \n",
       "2023-10-26  118.750000  204.830002  103.120003  30.450001  158.070007  \n",
       "2023-10-27  119.440002  207.179993  103.190002  29.450001  156.029999  \n",
       "2023-10-30  119.870003  209.770004  103.410004  29.980000  157.000000  \n",
       "\n",
       "[3480 rows x 503 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.32 s, sys: 366 ms, total: 3.68 s\n",
      "Wall time: 3.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker.strip())\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        # e.g. BRK.B => BRK-B, BF.B => BF-B\n",
    "        ticker = ticker.replace('.', '-')\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = yf.download(ticker, start=start, end=end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "def compile_data():\n",
    "    with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for ticker in tqdm(tickers):\n",
    "        # e.g. BRK.B => BRK-B, BF.B => BF-B\n",
    "        ticker = ticker.replace('.', '-')\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "    display(main_df)\n",
    "    main_df.to_csv('sp500_joined_closes.csv')\n",
    "\n",
    "\n",
    "compile_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313ced4",
   "metadata": {},
   "source": [
    "In the next tutorial, we're going to attempt to see if we can quickly find any relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
