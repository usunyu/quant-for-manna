{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a9ce85",
   "metadata": {},
   "source": [
    "## Machine learning against S&P 500 company prices\n",
    "\n",
    "https://pythonprogramming.net/machine-learning-stock-prices-python-programming-for-finance/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793026e9",
   "metadata": {},
   "source": [
    "Hello and welcome to part 12 of the Python for Finance tutorial series. In the previous tutorial, we covered how to take our data and create featuresets and labels out of it, which we can then feed through a machine learning algorithm with the hope that it will learn to map relationships of existing price changes to future price changes for a company.\n",
    "\n",
    "Before we begin, our starting code up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02db72a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import requests\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def ticker_fmt(ticker: str) -> str:\n",
    "    # e.g. BRK.B => BRK-B, BF.B => BF-B\n",
    "    return ticker.replace('.', '-')\n",
    "\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker.strip())\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        ticker = ticker_fmt(ticker)\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = yf.download(ticker, start=start, end=end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "def compile_data():\n",
    "    with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for ticker in tqdm(tickers):\n",
    "        ticker = ticker_fmt(ticker)\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "    display(main_df)\n",
    "    main_df.to_csv('sp500_joined_closes.csv')\n",
    "\n",
    "\n",
    "def visualize_data():\n",
    "    df = pd.read_csv('sp500_joined_closes.csv')\n",
    "    df_corr = df.corr(numeric_only=True)\n",
    "    display(df_corr.head())\n",
    "    df_corr.to_csv('sp500corr.csv')\n",
    "    data1 = df_corr.values\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "\n",
    "    heatmap1 = ax1.pcolor(data1, cmap=plt.cm.RdYlGn)\n",
    "    fig1.colorbar(heatmap1)\n",
    "\n",
    "    ax1.set_xticks(np.arange(data1.shape[1]) + 0.5, minor=False)\n",
    "    ax1.set_yticks(np.arange(data1.shape[0]) + 0.5, minor=False)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.xaxis.tick_top()\n",
    "    column_labels = df_corr.columns\n",
    "    row_labels = df_corr.index\n",
    "    ax1.set_xticklabels(column_labels)\n",
    "    ax1.set_yticklabels(row_labels)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap1.set_clim(-1, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def process_data_for_labels(ticker):\n",
    "    hm_days = 7\n",
    "    df = pd.read_csv('sp500_joined_closes.csv', index_col=0)\n",
    "    tickers = df.columns.values.tolist()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    for i in range(1,hm_days+1):\n",
    "        df['{}_{}d'.format(ticker,i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker]\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    return tickers, df\n",
    "\n",
    "\n",
    "def buy_sell_hold(*args):\n",
    "    cols = [c for c in args]\n",
    "    requirement = 0.02\n",
    "    for col in cols:\n",
    "        if col > requirement:\n",
    "            return 1\n",
    "        if col < -requirement:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_featuresets(ticker):\n",
    "    tickers, df = process_data_for_labels(ticker)\n",
    "\n",
    "    df['{}_target'.format(ticker)] = list(map( buy_sell_hold,\n",
    "                                               df['{}_1d'.format(ticker)],\n",
    "                                               df['{}_2d'.format(ticker)],\n",
    "                                               df['{}_3d'.format(ticker)],\n",
    "                                               df['{}_4d'.format(ticker)],\n",
    "                                               df['{}_5d'.format(ticker)],\n",
    "                                               df['{}_6d'.format(ticker)],\n",
    "                                               df['{}_7d'.format(ticker)]))\n",
    "\n",
    "    vals = df['{}_target'.format(ticker)].values.tolist()\n",
    "    str_vals = [str(i) for i in vals]\n",
    "    print('Data spread:', Counter(str_vals))\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df_vals = df[[ticker for ticker in tickers]].pct_change()\n",
    "    df_vals = df_vals.replace([np.inf, -np.inf], 0)\n",
    "    df_vals.fillna(0, inplace=True)\n",
    "\n",
    "    X = df_vals.values\n",
    "    y = df['{}_target'.format(ticker)].values\n",
    "    return X, y, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f41c65a",
   "metadata": {},
   "source": [
    "We're going to add the following imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154edcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, neighbors, model_selection\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704811c7",
   "metadata": {},
   "source": [
    "Sklearn is a [machine learning](https://pythonprogramming.net/machine-learning-tutorial-python-introduction/) framework. If you don't have it, make sure you download it: `pip install scikit-learn`. The svm import is for a [Support Vector Machine](https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/), `cross_validation` will let us easily create shuffled training and testing samples, and neighbors is for [K Nearest Neighbors](https://pythonprogramming.net/k-nearest-neighbors-intro-machine-learning-tutorial/). Then, we're bringing in the `VotingClassifier` and `RandomForestClassifier`. The voting classifier is just what it sounds like. Basically, it's a classifier that will let us combine many classifiers, and allow them to each get a \"vote\" on what they think the class of the featuresets is. The [random forest classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) is just another classifier. We're going to use three classifiers in our voting classifier.\n",
    "\n",
    "***Note: The `cross_validation` module has been deprecated since version 0.18 of scikit-learn, which was released back in 2016. It has been replaced by the `model_selection` module.***\n",
    "\n",
    "We're ready to do some machine learning now, so let's start our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2797a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spread: Counter({'1': 1576, '-1': 1477, '0': 427})\n"
     ]
    }
   ],
   "source": [
    "# def do_ml(ticker):\n",
    "ticker = \"BAC\"\n",
    "X, y, df = extract_featuresets(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c6191",
   "metadata": {},
   "source": [
    "We've got our featuresets and labels, now we want to shuffle them up, train, and then test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e745d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n",
    "                                                                    y,\n",
    "                                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36158dc",
   "metadata": {},
   "source": [
    "What this does for us is shuffle our data (so its not in any specific order any more), and then create training and testing samples for us. We don't want to **test** this algorithm on the same data we trained against. If we did that, chances are we'd do a lot better than we would in reality. We want to test the algorithm on data that it's never seen before to see if we've actually got a model that works.\n",
    "\n",
    "Now we can choose from any of the classifiers we want, for now, let's do one for [K Nearest Neighbors](https://pythonprogramming.net/k-nearest-neighbors-intro-machine-learning-tutorial/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8899a5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = neighbors.KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3c31f",
   "metadata": {},
   "source": [
    "Now we can `fit(train)` the classifier on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1926c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa2b7f2",
   "metadata": {},
   "source": [
    "This line will take our `X` data, and fit to our `y` data, for each of the pairs of `X`'s and `y`'s that we have. Once that's done, we can test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4c906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206e476",
   "metadata": {},
   "source": [
    "This will take some featuresets, `X_test`, make a prediction, and see if it matches our labels, `y_test`. It will return to us the percentage accuracy in decimal form, where 1.0 is 100%, and 0.1 is 10% accurate. Now we can output some further useful information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39d87115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.37701149425287356\n",
      "predicted class counts: Counter({1: 420, -1: 268, 0: 182})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy:',confidence)\n",
    "predictions = clf.predict(X_test)\n",
    "print('predicted class counts:',Counter(predictions))\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4de07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ml(ticker):\n",
    "    X, y, df = extract_featuresets(ticker)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n",
    "                                                                        y,\n",
    "                                                                        test_size=0.25)\n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    confidence = clf.score(X_test, y_test)\n",
    "    print('accuracy:',confidence)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('predicted class counts:',Counter(predictions))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d36c6c",
   "metadata": {},
   "source": [
    "This will tell us what the accuracy was, then we can get the precitions of the `X_testdata`, and then output the distribution (using `Counter`), so we can see if our model is only classifying one class, which is something that can easily happen.\n",
    "\n",
    "If this model is indeed successful, we can save it with `pickle`, and load it at any time to feed it some featuresets and get a prediction out of it, with `clf.predict`, which will predict either a single value from a single featureset, or a list of values from a list of featuresets.\n",
    "\n",
    "Alright, we're ready for the moment of truth! What is our goal? Well, something that picks randomly should be about 33% accurate, since we have three total choices in theory, but actually it isn't likely that our model will be truly balanced. Let's see some examples, and just run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8504064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spread: Counter({'1': 1334, '-1': 1138, '0': 1008})\n",
      "accuracy: 0.364367816091954\n",
      "predicted class counts: Counter({0: 371, -1: 278, 1: 221})\n",
      "\n",
      "\n",
      "Data spread: Counter({'1': 1682, '-1': 1285, '0': 513})\n",
      "accuracy: 0.4160919540229885\n",
      "predicted class counts: Counter({1: 515, -1: 298, 0: 57})\n",
      "\n",
      "\n",
      "Data spread: Counter({'1': 1370, '-1': 1088, '0': 1022})\n",
      "accuracy: 0.35172413793103446\n",
      "predicted class counts: Counter({1: 438, 0: 271, -1: 161})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do_ml('XOM')\n",
    "do_ml('AAPL')\n",
    "do_ml('ABT') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a85bc9",
   "metadata": {},
   "source": [
    "So all of these are better than 33%, but the training data wasn't perfectly balanced either. For example, we can look at the first one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "182e6707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_ml('XOM')\n",
    "\n",
    "# Data spread: Counter({'1': 1713, '-1': 1456, '0': 1108})\n",
    "# accuracy: 0.375700934579\n",
    "# predicted class counts: Counter({0: 404, -1: 393, 1: 273})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c68bc",
   "metadata": {},
   "source": [
    "In this case, what if the model ONLY predicted \"buy?\" That would have been 1,713 correct / 4,277, which is actually a better score than we got. What about the other two? The second one, AAPL, is 49% accurate if it just predicts buy, at least on the training data. ABT is 37% accurate if it just does buy on the training data.\n",
    "\n",
    "So, while we're doing better than 33%, it's currently unclear if this model is better than just saying \"buy\" on everything. In actual trading, this all can change. This model is being penalized, for example, if it says something is a buy, expecting a 2% rise in 7 days, but that 2% rise doesn't happen until 8 days, and yet, the algorithm calls it either a buy or hold along the way. In actual trading, this would still be fine. The same is true if this model turned out to be highly accurate. Actually trading a model can be a completely different thing entirely.\n",
    "\n",
    "Next, let's try that voting classifer. So, rather than `clf = neighbors.KNeighborsClassifier()`, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fd706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = VotingClassifier([('lsvc',svm.LinearSVC()),\n",
    "                        ('knn',neighbors.KNeighborsClassifier()),\n",
    "                        ('rfor',RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7b748e",
   "metadata": {},
   "source": [
    "Across the board, we have improvement! That's good to see. We're also notably using defaults on all of the algorithms. Each of these algorithms have quite a few parameters that we could spend a while tweaking to eek out a bit more performance and likely beat at the very least the odds of simply predicting \"buy\" on everything. That said, machine learning is a massive topic and it would take me months to go through everything. If you want to learn more about the algorithms yourself so you can tweak them, check out the [Machine Learning tutorial series](https://pythonprogramming.net/machine-learning-tutorial-python-introduction/). We cover a bunch of machine learning algorithms, how they work fundamentally, how to apply them, then how to make them ourselves in raw Python. By the time you get through that entire series, you should be very well equipped to wrangle all kinds of challenges with machine learning.\n",
    "\n",
    "Full code up to this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1f17884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data spread: Counter({'1': 1334, '-1': 1138, '0': 1008})\n",
      "accuracy: 0.40344827586206894\n",
      "predicted class counts: Counter({-1: 309, 0: 282, 1: 279})\n",
      "\n",
      "\n",
      "Data spread: Counter({'1': 1682, '-1': 1285, '0': 513})\n",
      "accuracy: 0.44482758620689655\n",
      "predicted class counts: Counter({1: 702, -1: 168})\n",
      "\n",
      "\n",
      "Data spread: Counter({'1': 1370, '-1': 1088, '0': 1022})\n",
      "accuracy: 0.39425287356321836\n",
      "predicted class counts: Counter({1: 498, -1: 201, 0: 171})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "import requests\n",
    "from collections import Counter\n",
    "from sklearn import svm, neighbors, model_selection\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "\n",
    "def ticker_fmt(ticker: str) -> str:\n",
    "    # e.g. BRK.B => BRK-B, BF.B => BF-B\n",
    "    return ticker.replace('.', '-')\n",
    "\n",
    "\n",
    "def save_sp500_tickers():\n",
    "    resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "    soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "    table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        tickers.append(ticker.strip())\n",
    "        \n",
    "    with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
    "        pickle.dump(tickers,f)\n",
    "        \n",
    "    return tickers\n",
    "\n",
    "\n",
    "def get_data_from_yahoo(reload_sp500=False):\n",
    "    if reload_sp500:\n",
    "        tickers = save_sp500_tickers()\n",
    "    else:\n",
    "        with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "            tickers = pickle.load(f)\n",
    "    if not os.path.exists('stock_dfs'):\n",
    "        os.makedirs('stock_dfs')\n",
    "\n",
    "    start = dt.datetime(2010, 1, 1)\n",
    "    end = dt.datetime.now()\n",
    "    for ticker in tickers:\n",
    "        ticker = ticker_fmt(ticker)\n",
    "        # just in case your connection breaks, we'd like to save our progress!\n",
    "        if not os.path.exists('stock_dfs/{}.csv'.format(ticker)):\n",
    "            df = yf.download(ticker, start=start, end=end)\n",
    "            df.to_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        else:\n",
    "            print('Already have {}'.format(ticker))\n",
    "\n",
    "\n",
    "def compile_data():\n",
    "    with open(\"sp500tickers.pickle\", \"rb\") as f:\n",
    "        tickers = pickle.load(f)\n",
    "\n",
    "    main_df = pd.DataFrame()\n",
    "\n",
    "    for ticker in tqdm(tickers):\n",
    "        ticker = ticker_fmt(ticker)\n",
    "        df = pd.read_csv('stock_dfs/{}.csv'.format(ticker))\n",
    "        df.set_index('Date', inplace=True)\n",
    "\n",
    "        df.rename(columns={'Adj Close': ticker}, inplace=True)\n",
    "        df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
    "\n",
    "        if main_df.empty:\n",
    "            main_df = df\n",
    "        else:\n",
    "            main_df = main_df.join(df, how='outer')\n",
    "\n",
    "    display(main_df)\n",
    "    main_df.to_csv('sp500_joined_closes.csv')\n",
    "\n",
    "\n",
    "def visualize_data():\n",
    "    df = pd.read_csv('sp500_joined_closes.csv')\n",
    "    df_corr = df.corr(numeric_only=True)\n",
    "    display(df_corr.head())\n",
    "    df_corr.to_csv('sp500corr.csv')\n",
    "    data1 = df_corr.values\n",
    "    fig1 = plt.figure()\n",
    "    ax1 = fig1.add_subplot(111)\n",
    "\n",
    "    heatmap1 = ax1.pcolor(data1, cmap=plt.cm.RdYlGn)\n",
    "    fig1.colorbar(heatmap1)\n",
    "\n",
    "    ax1.set_xticks(np.arange(data1.shape[1]) + 0.5, minor=False)\n",
    "    ax1.set_yticks(np.arange(data1.shape[0]) + 0.5, minor=False)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.xaxis.tick_top()\n",
    "    column_labels = df_corr.columns\n",
    "    row_labels = df_corr.index\n",
    "    ax1.set_xticklabels(column_labels)\n",
    "    ax1.set_yticklabels(row_labels)\n",
    "    plt.xticks(rotation=90)\n",
    "    heatmap1.set_clim(-1, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def process_data_for_labels(ticker):\n",
    "    hm_days = 7\n",
    "    df = pd.read_csv('sp500_joined_closes.csv', index_col=0)\n",
    "    tickers = df.columns.values.tolist()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    for i in range(1,hm_days+1):\n",
    "        df['{}_{}d'.format(ticker,i)] = (df[ticker].shift(-i) - df[ticker]) / df[ticker]\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    return tickers, df\n",
    "\n",
    "\n",
    "def buy_sell_hold(*args):\n",
    "    cols = [c for c in args]\n",
    "    requirement = 0.02\n",
    "    for col in cols:\n",
    "        if col > requirement:\n",
    "            return 1\n",
    "        if col < -requirement:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def extract_featuresets(ticker, verbose=True):\n",
    "    tickers, df = process_data_for_labels(ticker)\n",
    "\n",
    "    df['{}_target'.format(ticker)] = list(map( buy_sell_hold,\n",
    "                                               df['{}_1d'.format(ticker)],\n",
    "                                               df['{}_2d'.format(ticker)],\n",
    "                                               df['{}_3d'.format(ticker)],\n",
    "                                               df['{}_4d'.format(ticker)],\n",
    "                                               df['{}_5d'.format(ticker)],\n",
    "                                               df['{}_6d'.format(ticker)],\n",
    "                                               df['{}_7d'.format(ticker)]))\n",
    "\n",
    "    vals = df['{}_target'.format(ticker)].values.tolist()\n",
    "    str_vals = [str(i) for i in vals]\n",
    "    if verbose:\n",
    "        print('Data spread:', Counter(str_vals))\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    df_vals = df[[ticker for ticker in tickers]].pct_change()\n",
    "    df_vals = df_vals.replace([np.inf, -np.inf], 0)\n",
    "    df_vals.fillna(0, inplace=True)\n",
    "\n",
    "    X = df_vals.values\n",
    "    y = df['{}_target'.format(ticker)].values\n",
    "    return X, y, df\n",
    "\n",
    "\n",
    "def do_ml(ticker, verbose=True):\n",
    "    X, y, df = extract_featuresets(ticker, verbose=verbose)\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X,\n",
    "                                                                        y,\n",
    "                                                                        test_size=0.25)\n",
    "    clf = VotingClassifier([('lsvc',svm.LinearSVC()),\n",
    "                            ('knn',neighbors.KNeighborsClassifier(n_jobs=-1)),\n",
    "                            ('rfor',RandomForestClassifier(n_jobs=-1))])\n",
    "    clf.fit(X_train, y_train)\n",
    "    confidence = clf.score(X_test, y_test)\n",
    "    if verbose:\n",
    "        print('accuracy:',confidence)\n",
    "    predictions = clf.predict(X_test)\n",
    "    if verbose:\n",
    "        print('predicted class counts:',Counter(predictions))\n",
    "        print()\n",
    "        print()\n",
    "    return confidence\n",
    "\n",
    "\n",
    "# examples of running:\n",
    "_ = do_ml('XOM')\n",
    "_ = do_ml('AAPL')\n",
    "_ = do_ml('ABT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e9f42",
   "metadata": {},
   "source": [
    "We can also run this against all tickers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfba211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 503/503 [17:21<00:00,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:0.4463198245286163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "with open(\"sp500tickers.pickle\",\"rb\") as f:\n",
    "    tickers = pickle.load(f)\n",
    "\n",
    "accuracies = []\n",
    "for ticker in tqdm(tickers):\n",
    "    ticker = ticker_fmt(ticker)\n",
    "    accuracy = do_ml(ticker, verbose=False)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f\"Average accuracy:{mean(accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b14e912",
   "metadata": {},
   "source": [
    "This will take a while. I went ahead and did it, the result was an average accuracy of 46.279%. Not bad, but, from my looking around, the results are still questionable with any sort of strategy.\n",
    "\n",
    "In the next tutorials, we're going to be diving into testing trading strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
